{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36f64f4-a57b-40da-ae23-8a7303c89e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"✅ Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d2614f-a66b-4271-90fa-e2bc88ec2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training\n",
    "X_train = pd.DataFrame({\n",
    "    \"wave_height\": [1, 2, 3, 4, 5],\n",
    "    \"wind_speed\": [10, 20, 30, 40, 50]\n",
    "})\n",
    "y_train = [0, 0, 1, 1, 1]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction (with same column names)\n",
    "X_new = pd.DataFrame({\n",
    "    \"wave_height\": [6],\n",
    "    \"wind_speed\": [60]\n",
    "})\n",
    "print(model.predict(X_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e41e9bc-759a-4bb0-af8a-9ed74a5de495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alert_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    \"wave_height\": [1, 2, 3, 6, 7, 8],\n",
    "    \"wind_speed\": [10, 20, 25, 55, 65, 70],\n",
    "    \"alert\": [0, 0, 0, 1, 1, 1]\n",
    "})\n",
    "\n",
    "X = data[[\"wave_height\", \"wind_speed\"]]\n",
    "y = data[\"alert\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"alert_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27615572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved as final_training_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load all available datasets\n",
    "dfs = []\n",
    "\n",
    "# 1. Load station/environmental data\n",
    "if os.path.exists(\"Dataset_for_chatbot.csv\"):\n",
    "    station = pd.read_csv(\"Dataset_for_chatbot.csv\")\n",
    "    station[\"date\"] = pd.to_datetime(station[\"date\"], errors=\"coerce\").dt.date\n",
    "    dfs.append(station)\n",
    "\n",
    "# 2. Load weather data with timestamp\n",
    "if os.path.exists(\"weather_data_clean.csv\"):\n",
    "    weather = pd.read_csv(\"weather_data_clean.csv\")\n",
    "    if \"timestamp\" in weather.columns:\n",
    "        weather[\"date\"] = pd.to_datetime(weather[\"timestamp\"], errors=\"coerce\").dt.date\n",
    "    dfs.append(weather)\n",
    "\n",
    "# 3. Load rainfall data if available\n",
    "if os.path.exists(\"weather_data_with_rainfall.csv\"):\n",
    "    rain = pd.read_csv(\"weather_data_with_rainfall.csv\")\n",
    "    if \"timestamp\" in rain.columns:\n",
    "        rain[\"date\"] = pd.to_datetime(rain[\"timestamp\"], errors=\"coerce\").dt.date\n",
    "    dfs.append(rain)\n",
    "\n",
    "# 4. Merge all on 'date' and 'city' where possible\n",
    "from functools import reduce\n",
    "\n",
    "def merge_dfs(left, right):\n",
    "    common_cols = [col for col in [\"date\", \"city\"] if col in left.columns and col in right.columns]\n",
    "    if common_cols:\n",
    "        return pd.merge(left, right, on=common_cols, how=\"outer\")\n",
    "    else:\n",
    "        return pd.merge(left, right, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "final = reduce(merge_dfs, dfs)\n",
    "\n",
    "# Remove duplicate columns if any (from merges)\n",
    "final = final.loc[:,~final.columns.duplicated()]\n",
    "\n",
    "# Save the combined dataset\n",
    "final.to_csv(\"final_training_dataset.csv\", index=False)\n",
    "print(\"Combined dataset saved as final_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5736355-9ced-44ee-93f9-7074cad52bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       337\n",
      "         1.0       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00       341\n",
      "   macro avg       1.00      1.00      1.00       341\n",
      "weighted avg       1.00      1.00      1.00       341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"final_training_dataset.csv\")\n",
    "\n",
    "# Drop columns not useful for training (IDs, timestamps, text, city, etc.)\n",
    "drop_cols = [col for col in df.columns if\n",
    "             'timestamp' in col or\n",
    "             'weather' in col or\n",
    "             'station_id' in col or\n",
    "             'city' in col or\n",
    "             'date' in col or\n",
    "             'longitude' in col or\n",
    "             'latitude' in col]\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Drop rows with missing target\n",
    "df = df.dropna(subset=['anomaly'])\n",
    "\n",
    "# Fill missing values in features\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Keep only numeric columns for features\n",
    "X = df.drop(columns=['anomaly'])\n",
    "X = X.select_dtypes(include=['number'])\n",
    "y = df['anomaly']\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ac84a-4899-473b-a71f-757c912b4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "app = Flask(__name__)\n",
    "file_path = 'final_training_dataset.csv'\n",
    "# Load your dataset once (or query your DB)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "@app.route('/check_anomaly', methods=['POST'])\n",
    "def check_anomaly():\n",
    "    user_lat = float(request.json['latitude'])\n",
    "    user_lon = float(request.json['longitude'])\n",
    "    radius_km = 10  # Alert radius\n",
    "\n",
    "    # Filter for anomalies\n",
    "    anomalies = df[df['anomaly'] == 1]\n",
    "\n",
    "    # Check for nearby anomalies\n",
    "    for _, row in anomalies.iterrows():\n",
    "        anomaly_loc = (row['latitude'], row['longitude'])\n",
    "        user_loc = (user_lat, user_lon)\n",
    "        if geodesic(anomaly_loc, user_loc).km <= radius_km:\n",
    "            return jsonify({\"alert\": True, \"message\": \"Anomaly detected nearby!\"})\n",
    "\n",
    "    return jsonify({\"alert\": False, \"message\": \"No anomalies nearby.\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dea22ab-4c62-428b-8cf2-75ac2deb6ad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install dependencies as needed:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pip install kagglehub[pandas-datasets]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleDatasetAdapter\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Set the path to the file you'd like to load\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"weatherHistory.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"muthuj7/weather-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9dcf44-bc49-49c0-b7cb-f347feabd925",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39mTraceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Install dependencies as needed:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pip install kagglehub[pandas-datasets]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleDatasetAdapter\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Set the path to the file you'd like to load\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"weatherHistory.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"muthuj7/weather-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afe2bd4-f7aa-4473-b3f1-c5c254e44d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Using cached kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b27be61-8278-499d-8d71-f0102ac38e51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error reading file: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:91\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     result = \u001b[43mread_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_build_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Download and load the dataset with encoding fix\u001b[39;00m\n\u001b[32m      7\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mweatherHistory.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mKaggleDatasetAdapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPANDAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmuthuj7/weather-dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# or \"cp1252\"\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_bad_lines\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# skip problematic lines\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst 5 records:\u001b[39m\u001b[33m\"\u001b[39m, df.head())\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load your trained model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\datasets.py:141\u001b[39m, in \u001b[36mdataset_load\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.PANDAS:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpandas_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_pandas_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.POLARS:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolars_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:97\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     96\u001b[39m     read_error_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError reading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(read_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mValueError\u001b[39m: Error reading file: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Download and load the dataset with encoding fix\n",
    "file_path = \"weatherHistory.csv\"\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"muthuj7/weather-dataset\",\n",
    "    file_path,\n",
    "    pandas_kwargs={\n",
    "        \"encoding\": \"latin1\",      # or \"cp1252\"\n",
    "        \"on_bad_lines\": \"skip\"     # skip problematic lines\n",
    "    }\n",
    ")\n",
    "print(\"First 5 records:\", df.head())\n",
    "\n",
    "# Load your trained model\n",
    "model = joblib.load(\"alert_model.pkl\")\n",
    "\n",
    "# Prepare the features for prediction (adjust column names as needed)\n",
    "features = [\n",
    "    'water_level_m',\n",
    "    'wind_speed_m_s',\n",
    "    'air_pressure_hpa',\n",
    "    'chlorophyll_mg_m3',\n",
    "    'rainfall'\n",
    "]\n",
    "\n",
    "# If your dataset uses different column names, rename or select accordingly\n",
    "# Example: df = df.rename(columns={\"Wind Speed (km/h)\": \"wind_speed_m_s\", ...})\n",
    "\n",
    "# Fill missing values with mean (or as appropriate)\n",
    "X = df[features].fillna(df[features].mean())\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = model.predict(X)\n",
    "probabilities = model.predict_proba(X)[:, 1]  # Probability of anomaly\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df['anomaly_pred'] = predictions\n",
    "df['anomaly_prob'] = probabilities\n",
    "\n",
    "print(df[['anomaly_pred', 'anomaly_prob']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75b1c1-8a18-4517-84b9-07711ee4b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset (robust options for messy CSVs)\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add a synthetic 'region' column with random Indian cities\n",
    "indian_cities = [\n",
    "    'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata',\n",
    "    'Hyderabad', 'Pune', 'Ahmedabad', 'Jaipur', 'Lucknow'\n",
    "]\n",
    "df['region'] = np.random.choice(indian_cities, size=len(df))\n",
    "\n",
    "# One-hot encode the 'region' column\n",
    "df = pd.get_dummies(df, columns=['region'])\n",
    "\n",
    "# Choose a target variable to predict (e.g., 'Temperature (C)')\n",
    "target = 'Temperature (C)'\n",
    "\n",
    "# Select only numeric columns for features (plus one-hot region columns)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "features = numeric_cols\n",
    "\n",
    "# Drop rows with missing values in features/target\n",
    "df = df.dropna(subset=features + [target])\n",
    "\n",
    "# Prepare data for training\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# Predict for a random region in India\n",
    "random_region = random.choice(indian_cities)\n",
    "# Create a sample input (use the first row of X_test and set region columns)\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "for city in indian_cities:\n",
    "    col_name = f'region_{city}'\n",
    "    if col_name in sample.columns:\n",
    "        sample[col_name] = 1 if city == random_region else 0\n",
    "\n",
    "sample_pred = model.predict(sample)\n",
    "print(f\"Predicted temperature for {random_region}: {sample_pred[0]:.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0870713-a224-4839-965e-42cf14e2c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset (robust options for messy CSVs)\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add a synthetic 'region' column with random Indian cities\n",
    "indian_cities = [\n",
    "    'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata',\n",
    "    'Hyderabad', 'Pune', 'Ahmedabad', 'Jaipur', 'Lucknow'\n",
    "]\n",
    "df['region'] = np.random.choice(indian_cities, size=len(df))\n",
    "\n",
    "# One-hot encode the 'region' column\n",
    "df = pd.get_dummies(df, columns=['region'])\n",
    "\n",
    "# Choose a target variable to predict (e.g., 'Temperature (C)')\n",
    "target = 'Temperature (C)'\n",
    "\n",
    "# Select only numeric columns for features (plus one-hot region columns)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "features = numeric_cols\n",
    "\n",
    "# Drop rows with missing values in features/target\n",
    "df = df.dropna(subset=features + [target])\n",
    "\n",
    "# Prepare data for training\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# Predict for a random region in India\n",
    "random_region = random.choice(indian_cities)\n",
    "# Create a sample input (use the first row of X_test and set region columns)\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "for city in indian_cities:\n",
    "    col_name = f'region_{city}'\n",
    "    if col_name in sample.columns:\n",
    "        sample[col_name] = 1 if city == random_region else 0\n",
    "\n",
    "sample_pred = model.predict(sample)\n",
    "print(f\"Predicted temperature for {random_region}: {sample_pred[0]:.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c667227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset (robust options for messy CSVs)\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add a synthetic 'region' column with random Indian cities\n",
    "indian_cities = [\n",
    "    'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata',\n",
    "    'Hyderabad', 'Pune', 'Ahmedabad', 'Jaipur', 'Lucknow'\n",
    "]\n",
    "df['region'] = np.random.choice(indian_cities, size=len(df))\n",
    "\n",
    "# One-hot encode the 'region' column\n",
    "df = pd.get_dummies(df, columns=['region'])\n",
    "\n",
    "# Choose a target variable to predict (e.g., 'Temperature (C)')\n",
    "target = 'Temperature (C)'\n",
    "\n",
    "# Select only numeric columns for features (plus one-hot region columns)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "features = numeric_cols\n",
    "\n",
    "# Drop rows with missing values in features/target\n",
    "df = df.dropna(subset=features + [target])\n",
    "\n",
    "# Prepare data for training\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# Predict for a random region in India\n",
    "random_region = random.choice(indian_cities)\n",
    "# Create a sample input (use the first row of X_test and set region columns)\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "for city in indian_cities:\n",
    "    col_name = f'region_{city}'\n",
    "    if col_name in sample.columns:\n",
    "        sample[col_name] = 1 if city == random_region else 0\n",
    "\n",
    "sample_pred = model.predict(sample)\n",
    "print(f\"Predicted temperature for {random_region}: {sample_pred[0]:.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf6b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c436965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region-specific weather forecast and event probability\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add synthetic region column if not present\n",
    "if 'region' not in df.columns:\n",
    "    indian_cities = [\n",
    "        'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata',\n",
    "        'Hyderabad', 'Pune', 'Ahmedabad', 'Jaipur', 'Lucknow'\n",
    "    ]\n",
    "    df['region'] = np.random.choice(indian_cities, size=len(df))\n",
    "\n",
    "# Choose region and target variable\n",
    "region = 'Mumbai'  # Change as needed\n",
    "target = 'Temperature (C)'  # Change to 'Precipitation (mm)' or other variable as needed\n",
    "\n",
    "# Filter for the selected region\n",
    "df_region = df[df['region'] == region]\n",
    "\n",
    "# One-hot encode region (if needed)\n",
    "if 'region' in df_region.columns:\n",
    "    df_region = pd.get_dummies(df_region, columns=['region'])\n",
    "\n",
    "# Select numeric features\n",
    "numeric_cols = df_region.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in numeric_cols:\n",
    "    numeric_cols.remove(target)\n",
    "features = numeric_cols\n",
    "\n",
    "# Drop missing values\n",
    "df_region = df_region.dropna(subset=features + [target])\n",
    "\n",
    "# Prepare data\n",
    "X = df_region[features]\n",
    "y = df_region[target]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Regression: Predict weather variable (e.g., temperature)\n",
    "reg = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"{region} {target} MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(f\"Predicted {target} for {region}: {y_pred[0]:.2f}\")\n",
    "\n",
    "# Classification: Probability of a weather event (e.g., rain)\n",
    "# Example: Predict if 'Precipitation (mm)' > 0 (chance of rain)\n",
    "event = 'Precipitation (mm)'\n",
    "if event in df_region.columns:\n",
    "    df_region['rain'] = (df_region[event] > 0).astype(int)\n",
    "    Xc = df_region[features]\n",
    "    yc = df_region['rain']\n",
    "    Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42)\n",
    "    clf = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "    clf.fit(Xc_train, yc_train)\n",
    "    rain_prob = clf.predict_proba(Xc_test)[0][1]\n",
    "    print(f\"Chance of rain in {region}: {rain_prob*100:.1f}%\")\n",
    "    print(\"Rain prediction accuracy:\", accuracy_score(yc_test, clf.predict(Xc_test)))\n",
    "else:\n",
    "    print(f\"Column '{event}' not found for rain probability.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd3fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict next likely weather record for a region (all columns)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add synthetic region if not present\n",
    "if 'region' not in df.columns:\n",
    "    indian_cities = [\n",
    "        'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Kolkata',\n",
    "        'Hyderabad', 'Pune', 'Ahmedabad', 'Jaipur', 'Lucknow'\n",
    "    ]\n",
    "    df['region'] = np.random.choice(indian_cities, size=len(df))\n",
    "\n",
    "region = 'Mumbai'  # Change as needed\n",
    "df_region = df[df['region'] == region].copy()\n",
    "\n",
    "# Predict next date\n",
    "last_date = pd.to_datetime(df_region['Formatted Date']).max()\n",
    "next_date = last_date + timedelta(days=1)\n",
    "next_date_str = next_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Prepare features and targets\n",
    "numeric_cols = [\n",
    "    'Temperature (C)', 'Apparent Temperature (C)', 'Humidity',\n",
    "    'Wind Speed (km/h)', 'Wind Bearing (degrees)', 'Visibility (km)',\n",
    "    'Loud Cover', 'Pressure (millibars)'\n",
    "]\n",
    "cat_cols = ['Summary', 'Precip Type', 'Daily Summary']\n",
    "\n",
    "# Predict numeric columns using regression\n",
    "predicted = {}\n",
    "for col in numeric_cols:\n",
    "    if col in df_region.columns:\n",
    "        X = np.arange(len(df_region)).reshape(-1, 1)\n",
    "        y = df_region[col].values\n",
    "        model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        predicted[col] = model.predict([[len(df_region)]])[0]\n",
    "    else:\n",
    "        predicted[col] = np.nan\n",
    "\n",
    "# Predict categorical columns using most frequent value\n",
    "for col in cat_cols:\n",
    "    if col in df_region.columns and not df_region[col].isnull().all():\n",
    "        predicted[col] = df_region[col].mode()[0]\n",
    "    else:\n",
    "        predicted[col] = \"\"\n",
    "\n",
    "# Predict Precip Type as most frequent non-null value\n",
    "if 'Precip Type' in df_region.columns and not df_region['Precip Type'].isnull().all():\n",
    "    predicted['Precip Type'] = df_region['Precip Type'].dropna().mode()[0]\n",
    "\n",
    "# Format output\n",
    "forecast = {\n",
    "    \"Formatted Date\": next_date_str,\n",
    "    \"Summary\": predicted['Summary'],\n",
    "    \"Precip Type\": predicted['Precip Type'],\n",
    "    \"Temperature (C)\": round(predicted['Temperature (C)'], 2),\n",
    "    \"Apparent Temperature (C)\": round(predicted['Apparent Temperature (C)'], 2),\n",
    "    \"Humidity\": round(predicted['Humidity'], 2),\n",
    "    \"Wind Speed (km/h)\": round(predicted['Wind Speed (km/h)'], 2),\n",
    "    \"Wind Bearing (degrees)\": round(predicted['Wind Bearing (degrees)'], 2),\n",
    "    \"Visibility (km)\": round(predicted['Visibility (km)'], 2),\n",
    "    \"Loud Cover\": round(predicted['Loud Cover'], 2),\n",
    "    \"Pressure (millibars)\": round(predicted['Pressure (millibars)'], 2),\n",
    "    \"Daily Summary\": predicted['Daily Summary']\n",
    "}\n",
    "\n",
    "print(\"Next likely weather record for\", region)\n",
    "for k, v in forecast.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e0ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5db621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict weather for user at a specific location (lat/lon)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# User's location (replace with actual user input)\n",
    "user_lat = 19.0760   # Example: Mumbai latitude\n",
    "user_lon = 72.8777   # Example: Mumbai longitude\n",
    "\n",
    "# City coordinates (add more as needed)\n",
    "city_coords = {\n",
    "    'Mumbai': (19.0760, 72.8777),\n",
    "    'Delhi': (28.7041, 77.1025),\n",
    "    'Bangalore': (12.9716, 77.5946),\n",
    "    'Chennai': (13.0827, 80.2707),\n",
    "    'Kolkata': (22.5726, 88.3639),\n",
    "    'Hyderabad': (17.3850, 78.4867),\n",
    "    'Pune': (18.5204, 73.8567),\n",
    "    'Ahmedabad': (23.0225, 72.5714),\n",
    "    'Jaipur': (26.9124, 75.7873),\n",
    "    'Lucknow': (26.8467, 80.9462)\n",
    "}\n",
    "\n",
    "# Find nearest city\n",
    "min_dist = float('inf')\n",
    "nearest_city = None\n",
    "for city, coords in city_coords.items():\n",
    "    dist = geodesic((user_lat, user_lon), coords).km\n",
    "    if dist < min_dist:\n",
    "        min_dist = dist\n",
    "        nearest_city = city\n",
    "\n",
    "print(f\"Nearest city to user: {nearest_city} (distance: {min_dist:.2f} km)\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\n",
    "    \"weatherHistory.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Add synthetic region if not present\n",
    "if 'region' not in df.columns:\n",
    "    df['region'] = np.random.choice(list(city_coords.keys()), size=len(df))\n",
    "\n",
    "df_region = df[df['region'] == nearest_city].copy()\n",
    "\n",
    "# Predict next date\n",
    "last_date = pd.to_datetime(df_region['Formatted Date'], utc=True).max()\n",
    "next_date = last_date + timedelta(days=1)\n",
    "next_date_str = next_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Prepare features and targets\n",
    "numeric_cols = [\n",
    "    'Temperature (C)', 'Apparent Temperature (C)', 'Humidity',\n",
    "    'Wind Speed (km/h)', 'Wind Bearing (degrees)', 'Visibility (km)',\n",
    "    'Loud Cover', 'Pressure (millibars)'\n",
    "]\n",
    "cat_cols = ['Summary', 'Precip Type', 'Daily Summary']\n",
    "\n",
    "# Predict numeric columns using regression\n",
    "predicted = {}\n",
    "for col in numeric_cols:\n",
    "    if col in df_region.columns:\n",
    "        X = np.arange(len(df_region)).reshape(-1, 1)\n",
    "        y = df_region[col].values\n",
    "        model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        predicted[col] = model.predict([[len(df_region)]])[0]\n",
    "    else:\n",
    "        predicted[col] = np.nan\n",
    "\n",
    "# Predict categorical columns using most frequent value\n",
    "for col in cat_cols:\n",
    "    if col in df_region.columns and not df_region[col].isnull().all():\n",
    "        predicted[col] = df_region[col].mode()[0]\n",
    "    else:\n",
    "        predicted[col] = \"\"\n",
    "\n",
    "# Predict Precip Type as most frequent non-null value\n",
    "if 'Precip Type' in df_region.columns and not df_region['Precip Type'].isnull().all():\n",
    "    predicted['Precip Type'] = df_region['Precip Type'].dropna().mode()[0]\n",
    "\n",
    "# Format output\n",
    "forecast = {\n",
    "    \"Formatted Date\": next_date_str,\n",
    "    \"Summary\": predicted['Summary'],\n",
    "    \"Precip Type\": predicted['Precip Type'],\n",
    "    \"Temperature (C)\": round(predicted['Temperature (C)'], 2),\n",
    "    \"Apparent Temperature (C)\": round(predicted['Apparent Temperature (C)'], 2),\n",
    "    \"Humidity\": round(predicted['Humidity'], 2),\n",
    "    \"Wind Speed (km/h)\": round(predicted['Wind Speed (km/h)'], 2),\n",
    "    \"Wind Bearing (degrees)\": round(predicted['Wind Bearing (degrees)'], 2),\n",
    "    \"Visibility (km)\": round(predicted['Visibility (km)'], 2),\n",
    "    \"Loud Cover\": round(predicted['Loud Cover'], 2),\n",
    "    \"Pressure (millibars)\": round(predicted['Pressure (millibars)'], 2),\n",
    "    \"Daily Summary\": predicted['Daily Summary']\n",
    "}\n",
    "\n",
    "print(f\"Weather forecast for user at ({user_lat}, {user_lon}) - {nearest_city}:\")\n",
    "for k, v in forecast.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdadfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
